{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5023457",
   "metadata": {},
   "source": [
    "# EXP 1 TOKENIZATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95bbe7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Own Function: ['Hey,', 'I', 'am', 'amulya', 'and', \"it's\", 'a', 'rainy', 'day.', 'How', 'are', 'you?', '#ok']\n",
      "Using NLTK Sentence Tokenization: [\"Hey, I am amulya and it's a rainy day.\", 'How are you?', '#ok']\n",
      "Using NLTK Word Tokenization: ['Hey', ',', 'I', 'am', 'amulya', 'and', 'it', \"'s\", 'a', 'rainy', 'day', '.', 'How', 'are', 'you', '?', '#', 'ok']\n",
      "Using NLTK WordPunctTokenizer: ['Hey', ',', 'I', 'am', 'amulya', 'and', 'it', \"'\", 's', 'a', 'rainy', 'day', '.', 'How', 'are', 'you', '?', '#', 'ok']\n",
      "Using Regex: ['Hey', 'I', 'am', 'amulya', 'and', \"it's\", 'a', 'rainy', 'day', 'How', 'are', 'you', 'ok']\n",
      "Using NLTK Tweet Tokenization: ['Hey', ',', 'I', 'am', 'amulya', 'and', \"it's\", 'a', 'rainy', 'day', '.', 'How', 'are', 'you', '?', '#ok']\n",
      "Using NLTK TreeBank Word Tokenization: ['Hey', ',', 'I', 'am', 'amulya', 'and', 'it', \"'s\", 'a', 'rainy', 'day.', 'How', 'are', 'you', '?', '#', 'ok']\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries and methods\n",
    "from typing import List \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize,   WordPunctTokenizer, TweetTokenizer, TreebankWordTokenizer,RegexpTokenizer\n",
    "import re\n",
    "#Own Function to convert sentence into words\n",
    "def convert_to_words(s: str) -> List[str]:\n",
    "    '''Returns the Tokenize list of string that split when \" \" is encounter\n",
    "    Params:\n",
    "        s: input string: str\n",
    "    Returns:\n",
    "        List[int]: tokenize list of string \n",
    "    '''\n",
    "    ans = []\n",
    "    s += ' '\n",
    "    temp = ''\n",
    "    for i in s:\n",
    "        if i == ' ':\n",
    "            ans.append(temp)\n",
    "            temp = ''\n",
    "        else:\n",
    "            temp += i\n",
    "    return ans\n",
    "# #Regex function to tokenize\n",
    "# def regex(s: str) -> List[str]:\n",
    "#     word_regex_improved = r\"(\\w[\\w']*\\w|\\w)\"\n",
    "#     word_matcher = re.compile(word_regex_improved)\n",
    "#     return word_matcher.findall(s)\n",
    "\n",
    "\n",
    "\n",
    "text = \"Hey, I am amulya and it's a rainy day. How are you? #ok\"\n",
    "print(f'Using Own Function: {convert_to_words(text)}')\n",
    "print(f'Using NLTK Sentence Tokenization: {sent_tokenize(text)}')\n",
    "print(f'Using NLTK Word Tokenization: {word_tokenize(text)}')\n",
    "print(f'Using NLTK WordPunctTokenizer: {WordPunctTokenizer().tokenize(text)}')\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "print(f'Using Regex: {tokenizer.tokenize(text)}')\n",
    "print(f'Using NLTK Tweet Tokenization: {TweetTokenizer().tokenize(text)}')\n",
    "print(f'Using NLTK TreeBank Word Tokenization: {TreebankWordTokenizer().tokenize(text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2675426b",
   "metadata": {},
   "source": [
    "# EXPERIMENT 2 STEMMING AND LEMMITIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce5176",
   "metadata": {},
   "source": [
    "#PORTER STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ee56fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming using Porter Stemming:-\n",
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "programs----->program\n",
      "history----->histori\n",
      "finally----->final\n",
      "finalized----->final\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries and methods \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "stemmer = PorterStemmer() #creating a object of the Porter Class \n",
    "\n",
    "words = [\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\"]\n",
    "# text = input(\"Enter an sentence: \")\n",
    "# tokenized_text = word_tokenize(text)\n",
    "print(\"Stemming using Porter Stemming:-\")\n",
    "# for word in tokenized_text:\n",
    "#     print(f'{word}----->{stemmer.stem(word)}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word}----->{stemmer.stem(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa806d1c",
   "metadata": {},
   "source": [
    "#LANCASTER STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b109464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming using Lancaster Stemming:-\n",
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eat\n",
      "writing----->writ\n",
      "writes----->writ\n",
      "programming----->program\n",
      "programs----->program\n",
      "history----->hist\n",
      "finally----->fin\n",
      "finalized----->fin\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "# text = input(\"Enter an sentence: \")\n",
    "# tokenized_text = word_tokenize(text)\n",
    "print(\"Stemming using Lancaster Stemming:-\")\n",
    "# for word in tokenized_text:\n",
    "#     print(f'{word}----->{lancaster.stem(word)}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word}----->{lancaster.stem(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d056e1",
   "metadata": {},
   "source": [
    "#RegexpStemmer class\n",
    "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46b0e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming using Regexp Stemming:-\n",
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "writing----->writ\n",
      "writes----->write\n",
      "programming----->programm\n",
      "programs----->program\n",
      "history----->history\n",
      "finally----->finally\n",
      "finalized----->finalized\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer('ing|s$|e$|able$', min=4) #minimum no of word for applying regex\n",
    "# text = input(\"Enter an sentence: \")\n",
    "# tokenized_text = word_tokenize(text)\n",
    "print(\"Stemming using Regexp Stemming:-\")\n",
    "# for word in tokenized_text:\n",
    "#     print(f'{word}----->{reg_stemmer.stem(word)}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word}----->{reg_stemmer.stem(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e781e",
   "metadata": {},
   "source": [
    "#Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca6491c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming using Snowball Stemming:-\n",
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "programs----->program\n",
      "history----->histori\n",
      "finally----->final\n",
      "finalized----->final\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowballstemmer = SnowballStemmer('english', ignore_stopwords=False)\n",
    "# text = input(\"Enter an sentence: \")\n",
    "# tokenized_text = word_tokenize(text)\n",
    "print(\"Stemming using Snowball Stemming:-\")\n",
    "# for word in tokenized_text:\n",
    "#     print(f'{word}----->{snowballstemmer.stem(word)}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word}----->{snowballstemmer.stem(word)}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a03ed34",
   "metadata": {},
   "source": [
    "Wordnet Lemmatizer\n",
    "Lemmatization technique is like stemming. The output we will get after lemmatization is called 'lemma', which is a root word rather than root stem. the output of stemming. After lemmatization we will be getting valid word that means the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09eb7d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\MY\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fdef601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    " \n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a611a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing using WordNetLemmatizer Lammetizer:-\n",
      "eating----->eating\n",
      "eats----->eats\n",
      "eaten----->eaten\n",
      "writing----->writing\n",
      "writes----->writes\n",
      "programming----->programming\n",
      "programs----->program\n",
      "history----->history\n",
      "finally----->finally\n",
      "finalized----->finalized\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lammetizer = WordNetLemmatizer()\n",
    "# text = input(\"Enter an sentence: \")\n",
    "# tokenized_text = word_tokenize(text)\n",
    "print(\"Lemmatizing using WordNetLemmatizer Lammetizer:-\")\n",
    "# for word in tokenized_text:\n",
    "#    print(f'{word}----->{lammetizer.lemmatize(word)}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word}----->{lammetizer.lemmatize(word)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d04f7bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eat\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "programs----->program\n",
      "history----->history\n",
      "finally----->finally\n",
      "finalized----->finalize\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + '----->' + lammetizer.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68acf704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lammetizer.lemmatize('better', pos='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e3751",
   "metadata": {},
   "source": [
    "# EXPERIMENT 3  STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "829f3e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\MY\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d331abcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Sentence: This is a sample sentence, showing off the stop words filtration.\n",
      "Tokenized Sentence: ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "After removing stop words: ['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "print(f\"Sample Sentence: {example_sent}\")\n",
    "print(f\"Tokenized Sentence: {word_tokens}\")\n",
    "print(f\"After removing stop words: {filtered_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb6ba7",
   "metadata": {},
   "source": [
    "CREATING OWN STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85f7a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Sentence: This is a sample sentence, showing off the stop words filtration.\n",
      "Tokenized Sentence: ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "After removing stop words: ['This', 'sample', 'sentence', ',', 'showing', 'the', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stopwords = {'hers', 'ourselves', 'is', 'she', 'her', 'in', 'if', 'a', 'my', 'off'}\n",
    "word_tokenized = word_tokenize(example_sent)\n",
    "stop_word_remover = [w for w in word_tokenized if not w in stopwords]\n",
    "print(f\"Sample Sentence: {example_sent}\")\n",
    "print(f\"Tokenized Sentence: {word_tokenized}\")\n",
    "print(f\"After removing stop words: {stop_word_remover}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0a66083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This is an example sentence to demonstrate stopwords removal.\n",
      "Filtered Text: This example sentence demonstrate stopwords removal .\n"
     ]
    }
   ],
   "source": [
    "def remove_custom_stopwords(text, custom_stopwords):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove custom stopwords from the text\n",
    "    filtered_words = [word for word in words if word.lower() not in custom_stopwords]\n",
    "    \n",
    "    # Join the filtered words back into a sentence\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text\n",
    "\n",
    "# Example usage\n",
    "text = \"This is an example sentence to demonstrate stopwords removal.\"\n",
    "custom_stopwords = ['is', 'an', 'to']\n",
    "filtered_text = remove_custom_stopwords(text, custom_stopwords)\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Filtered Text:\", filtered_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2452c6",
   "metadata": {},
   "source": [
    "\n",
    "# EXPERIMENT 4 NGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd03df3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the sentence: My name is amulya maurya.\n",
      "Enter the value of n: 3\n",
      "Original Text: My name is amulya maurya.\n",
      "3-grams: ['my name is', 'name is amulya', 'is amulya maurya']\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    # Convert the text to lowercase and remove any punctuation\n",
    "    text = text.lower()\n",
    "    text = ''.join(c for c in text if c.isalnum() or c.isspace())\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Generate n-grams from the words\n",
    "    ngrams_list = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = ' '.join(words[i:i+n])\n",
    "        ngrams_list.append(ngram)\n",
    "\n",
    "    return ngrams_list\n",
    "\n",
    "# Example usage\n",
    "text = input(\"Enter the sentence: \")\n",
    "n = int(input(\"Enter the value of n: \"))\n",
    "\n",
    "result = generate_ngrams(text, n)\n",
    "print(\"Original Text:\", text)\n",
    "print(f\"{n}-grams:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66439be4",
   "metadata": {},
   "source": [
    "# Experiment 5 n gram smoothing add one method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bfb3cc",
   "metadata": {},
   "source": [
    "P(w_n | w_1, w_2, ..., w_{n-1}) = (Count(w_1, w_2, ..., w_n) + 1) / (Count(w_1, w_2, ..., w_{n-1}) + V)\n",
    "\n",
    "\n",
    "Laplace smoothing, also known as add-one smoothing, is a simple technique used in n-gram language modeling to address the problem of zero probabilities for unseen n-grams. It is named after the mathematician Pierre-Simon Laplace.\n",
    "\n",
    "In n-gram language models, probabilities are calculated based on the frequency of n-grams observed in the training data. However, there may be cases where certain n-grams do not appear in the training data, resulting in zero probabilities when using them for language modeling tasks.\n",
    "\n",
    "Laplace smoothing addresses this issue by adding a small constant value (usually 1) to both the numerator and denominator of the probability calculation for each n-gram. This ensures that even unseen n-grams have a non-zero probability assigned to them.\n",
    "\n",
    "The formula for calculating the Laplace-smoothed probability of an n-gram is:\n",
    "P(w_n | w_1, w_2, ..., w_{n-1}) = (Count(w_1, w_2, ..., w_n) + 1) / (Count(w_1, w_2, ..., w_{n-1}) + V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ddec2798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: This is a good book\n",
      "Enter the value of n for n-grams: 2\n",
      "N-gram: ('this', 'is')\n",
      "Smoothed Probability: 0.3333333333333333\n",
      "---\n",
      "N-gram: ('is', 'a')\n",
      "Smoothed Probability: 0.3333333333333333\n",
      "---\n",
      "N-gram: ('a', 'good')\n",
      "Smoothed Probability: 0.3333333333333333\n",
      "---\n",
      "N-gram: ('good', 'book')\n",
      "Smoothed Probability: 0.3333333333333333\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train_ngram_model(corpus, n):\n",
    "    ngrams_counts = defaultdict(int)\n",
    "    context_counts = defaultdict(int)\n",
    "\n",
    "    # Tokenize the corpus into n-grams\n",
    "    ngrams = [tuple(corpus[i:i+n]) for i in range(len(corpus)-n+1)]\n",
    "\n",
    "    # Count occurrences of n-grams and their contexts\n",
    "    for ngram in ngrams:\n",
    "        context = ngram[:-1]\n",
    "        ngrams_counts[ngram] += 1\n",
    "        context_counts[context] += 1\n",
    "\n",
    "    return ngrams_counts, context_counts\n",
    "\n",
    "def calculate_smoothed_probability(ngram, ngrams_counts, context_counts, vocabulary_size):\n",
    "    context = ngram[:-1]\n",
    "    numerator = ngrams_counts[ngram] + 1\n",
    "    denominator = context_counts[context] + vocabulary_size\n",
    "\n",
    "    return numerator / denominator\n",
    "\n",
    "# Get user input\n",
    "sentence = input(\"Enter a sentence: \")\n",
    "n = int(input(\"Enter the value of n for n-grams: \"))\n",
    "\n",
    "# Preprocess the sentence\n",
    "corpus = sentence.lower().split()\n",
    "\n",
    "# Train the n-gram model\n",
    "ngrams_counts, context_counts = train_ngram_model(corpus, n)\n",
    "\n",
    "# Calculate the smoothed probability for each n-gram\n",
    "vocabulary_size = len(set(corpus))\n",
    "probabilities = []\n",
    "\n",
    "for ngram in ngrams_counts.keys():\n",
    "    probability = calculate_smoothed_probability(ngram, ngrams_counts, context_counts, vocabulary_size)\n",
    "    probabilities.append((ngram, probability))\n",
    "\n",
    "# Print the n-grams and their corresponding probabilities\n",
    "for ngram, probability in probabilities:\n",
    "    print(\"N-gram:\", ngram)\n",
    "    print(\"Smoothed Probability:\", probability)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f902965",
   "metadata": {},
   "source": [
    "# Experiment 6 POS USING HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a97e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['the', 'red', 'apple', 'eats']\n",
      "Tags: ['Noun', 'Noun', 'Noun', 'Noun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import hmm\n",
    "\n",
    "# Prepare the training data\n",
    "tagged_sentences = [\n",
    "    [('apple', 'Noun'), ('eats', 'Verb'), ('red', 'Adjective')],\n",
    "    [('red', 'Adjective'), ('apple', 'Noun'), ('eats', 'Verb')]\n",
    "]\n",
    "\n",
    "# Create the training corpus\n",
    "train_corpus = [[(word, tag) for word, tag in sentence] for sentence in tagged_sentences]\n",
    "\n",
    "# Create and train the HMM model\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "model = trainer.train_supervised(train_corpus)\n",
    "\n",
    "# Perform POS tagging on a new sentence\n",
    "sentence = ['the', 'red', 'apple', 'eats']\n",
    "predicted_states = model.best_path(sentence)\n",
    "\n",
    "# Print the sentence and predicted tags\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Tags:\", predicted_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee35b1",
   "metadata": {},
   "source": [
    "# EXPERIMENT 9 CHUNKING TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c245931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S I/PRP (VP am/VBP (NP Amulya/NNP Maurya/NNP)))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import Tree\n",
    "\n",
    "# Define a sample sentence\n",
    "sentence = \"I am Amulya Maurya\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Define a chunk grammar using regular expressions\n",
    "chunk_grammar = r\"\"\"\n",
    "    NP: {<DT|JJ|NN.*>+}  # Noun phrase\n",
    "    VP: {<VB.*><NP|PP|CLAUSE>+$}  # Verb phrase\n",
    "    PP: {<IN><NP>}  # Prepositional phrase\n",
    "    CLAUSE: {<NP><VP>}  # Clause\n",
    "\"\"\"\n",
    "\n",
    "# Create a chunk parser\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "\n",
    "# Apply chunking to the part-of-speech tagged sentence\n",
    "chunked_sentence = chunk_parser.parse(pos_tags)\n",
    "\n",
    "# Print the chunked sentence\n",
    "print(chunked_sentence)\n",
    "\n",
    "# Draw and display the parse tree\n",
    "chunked_sentence.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16c02d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: neg\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "\n",
    "# Load the movie reviews dataset\n",
    "#nltk.download('movie_reviews')\n",
    "\n",
    "# Prepare the dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Rest of the code remains the same\n",
    "# ...\n",
    "\n",
    "# Extract features from the documents\n",
    "all_words = FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_set = featuresets[:1500]\n",
    "test_set = featuresets[1500:]\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Define a sample text\n",
    "text = \"I love this movie.\"\n",
    "\n",
    "# Tokenize the sample text\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Extract features from the tokens\n",
    "features = document_features(tokens)\n",
    "\n",
    "# Perform sentiment analysis using the trained classifier\n",
    "sentiment = classifier.classify(features)\n",
    "\n",
    "# Print the sentiment\n",
    "print(\"Sentiment:\", sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f0707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
